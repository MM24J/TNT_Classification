{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "78ee4eb0-52c8-41fd-b92c-8d9975e70f8d",
      "metadata": {
        "id": "78ee4eb0-52c8-41fd-b92c-8d9975e70f8d"
      },
      "source": [
        "In this project, you will create a deep learning model that detects the TNt tubes formed in between cells. Below is a brief description of what TNTs are:\n",
        "\n",
        "\"Tunneling nanotubes (TNTs) are elongated structures extending from and connecting cancer cell membranes. They permit the exchange of molecules, vesicles, and mitochondria, as well as genetic and metabolic signals that promote carcinogenesis. Given that they permit intercellular trafficking and communication, TNTs may serve as an important imaging biomarker of cancer cell response vs. resistance to therapy. On fluorescence imaging of cancer cell cultures, TNTs appear to be no thicker than 1 µm and vary in length from 10 to 100+ µm. TNTs can be spotted by a trained eye, but using human experts to obtain an accurate count and location of TNTs is a time-intensive process. A precise quantitative analysis of TNTs could aid in the objective assessment of cancer response to various therapeutic interventions.\"\n",
        "\n",
        "In this project, the original images were created by taking a grid of 5 × 5 tiled images, each measuring 1388 × 1040 pixels, and then stitching them together. This process resulted in shadows along the stitched edges, which significantly degraded the model performance at later stages. You may start from removing these shadows. To remove those shadows, you may use BaSiC, an image correction method for background and shading correction for image sequences, available as a Fiji/ImageJ. You may consider other packages/lobraries for this purpose. You may also consider using different filters. You may check the following link: https://www.youtube.com/watch?v=xCHbcVUCYBI. You may find very useful short videos in that channel for image processing.\n",
        "\n",
        "After preprocessing, you may want to divide the original image into smaller pieces. The original image in the training dataset was stitched together resulting in an image - size of 6283 × 4687 pixels. You can scan the images  with a sliding window of 512 × 512 pixels with a stride of 10 pixels, extracting patches containing the TNT regions using a bounding box. You may write a function that takes the window size as input. You may later create smaller images from 512x512 images using the same function.\n",
        "\n",
        "Once you form training and test datasets from smaller images, you may simply train a VGG model. You may try different models. you goal is to find the images that contain TNTs. You may create multiple models that are trained with images with different sizes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "546cced9-060e-40cf-83fc-2004238821c9",
      "metadata": {
        "id": "546cced9-060e-40cf-83fc-2004238821c9"
      },
      "source": [
        "# Image Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "6abf194f-e772-43c2-a74d-39c1b0c2b422",
      "metadata": {
        "id": "6abf194f-e772-43c2-a74d-39c1b0c2b422",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1ee05f9-082e-4c42-c085-6047e0216314"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define consistent paths\n",
        "import os\n",
        "import shutil\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Paths\n",
        "base_drive_path = \"/content/drive/MyDrive/Mentorship_DL_Module/Second Projects/TNT Project\"\n",
        "base_colab_path = \"/content/TNT_Project\"\n"
      ],
      "metadata": {
        "id": "nhFNhDo0ra1U"
      },
      "id": "nhFNhDo0ra1U",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d7a8c70f-40f1-4a19-9480-78487f385a89",
      "metadata": {
        "id": "d7a8c70f-40f1-4a19-9480-78487f385a89"
      },
      "source": [
        "This is the image with labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f16baa9-5538-4a7a-8ed0-a1c634a37dc8",
      "metadata": {
        "id": "9f16baa9-5538-4a7a-8ed0-a1c634a37dc8"
      },
      "source": [
        "![Screen Shot 2022-04-30 at 5.32.27 AM.png](attachment:27b423ed-9de7-4634-9288-52f227b53968.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53532c56-2a45-4055-9f12-0f10c5e41e87",
      "metadata": {
        "id": "53532c56-2a45-4055-9f12-0f10c5e41e87"
      },
      "source": [
        "This is the same image without labels."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa41aca1-44ea-44cd-b5cb-0123efee3d21",
      "metadata": {
        "id": "fa41aca1-44ea-44cd-b5cb-0123efee3d21"
      },
      "source": [
        "![Screen Shot 2022-04-30 at 5.33.17 AM.png](attachment:a426a0b5-5596-4037-a92b-24753646b87a.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "057bfaa6-3186-4473-975c-908c514af6f4",
      "metadata": {
        "id": "057bfaa6-3186-4473-975c-908c514af6f4"
      },
      "source": [
        "- Create folders to save the images that are created by splitting the main image into smaller pieces\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create necessary directories\n",
        "labeled_dir = os.path.join(base_colab_path, \"labeled\")\n",
        "os.makedirs(os.path.join(labeled_dir, \"1\"), exist_ok=True)  # TNT patches\n",
        "os.makedirs(os.path.join(labeled_dir, \"0\"), exist_ok=True)  # No-TNT patches"
      ],
      "metadata": {
        "id": "sAyt1BoLwMnn"
      },
      "id": "sAyt1BoLwMnn",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy images from Drive\n",
        "shutil.copy(os.path.join(base_drive_path, \"m05.png\"), base_colab_path)\n",
        "shutil.copy(os.path.join(base_drive_path, \"m05-label.png\"), base_colab_path)\n"
      ],
      "metadata": {
        "id": "ZCgoIr1urTZB",
        "outputId": "58c8206a-a146-4a83-a273-d6d615ebff76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "id": "ZCgoIr1urTZB",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/TNT_Project/m05-label.png'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load images\n",
        "image_path = os.path.join(base_colab_path, \"m05.png\")  # Unlabeled image\n",
        "label_image_path = os.path.join(base_colab_path, \"m05-label.png\")  # Labeled image\n",
        "\n",
        "pil_image = Image.open(image_path).convert(\"L\")\n",
        "image = np.array(pil_image)\n",
        "label_image = np.array(Image.open(label_image_path).convert(\"L\"))"
      ],
      "metadata": {
        "id": "4poPZ27bQ_tz"
      },
      "id": "4poPZ27bQ_tz",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e9dc3929-cba0-40b9-899d-c9c06a5059fe",
      "metadata": {
        "id": "e9dc3929-cba0-40b9-899d-c9c06a5059fe"
      },
      "source": [
        "![Screen Shot 2022-04-30 at 5.39.51 AM.png](attachment:323c38bf-351c-4123-a434-622843c08e15.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81f38b1d-6fe9-4d33-b6be-521c030615fd",
      "metadata": {
        "id": "81f38b1d-6fe9-4d33-b6be-521c030615fd"
      },
      "source": [
        "- Create smaller pieces from the main image by moving a window along the image. Mark the pieces with TNTs as 1 and all others as 0. This will give you the labeled images for your classification model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define patch size and stride\n",
        "window_size, stride = 512, 512\n",
        "height, width = image.shape\n",
        "patch_count = 0"
      ],
      "metadata": {
        "id": "a1DuFjDPRKY2"
      },
      "id": "a1DuFjDPRKY2",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define patch size and stride\n",
        "window_size, stride = 512, 512  # You might want to experiment with different values\n",
        "height, width = image.shape\n",
        "patch_count = 0\n",
        "\n",
        "# Extract and label patches in a single loop\n",
        "for y in tqdm(range(0, height - window_size + 1, stride)):\n",
        "    for x in range(0, width - window_size + 1, stride):\n",
        "        # Extract patch from main image\n",
        "        patch = image[y:y+window_size, x:x+window_size]\n",
        "\n",
        "        # Extract corresponding label patch\n",
        "        label_patch = label_image[y:y+window_size, x:x+window_size]\n",
        "\n",
        "        # Calculate the percentage of white pixels (TNTs are white in the label image)\n",
        "        white_pixel_percentage = np.sum(label_patch == 255) / label_patch.size * 100\n",
        "\n",
        "        # Adjust the threshold as needed - This value might need fine-tuning\n",
        "        label_folder = \"1\" if white_pixel_percentage > 2 else \"0\"\n",
        "\n",
        "        # Save patch directly to labeled folder\n",
        "        patch_filename = f\"patch_{patch_count:06d}.png\"\n",
        "        patch_path = os.path.join(labeled_dir, label_folder, patch_filename)\n",
        "        cv2.imwrite(patch_path, patch)\n",
        "\n",
        "        patch_count += 1\n",
        "\n",
        "print(f\"✅ Successfully extracted and labeled {patch_count} patches!\")\n"
      ],
      "metadata": {
        "id": "BM0V3-mrxVy6",
        "outputId": "52a22c8b-3997-42b7-b922-0934c3a61ef6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "BM0V3-mrxVy6",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9/9 [00:00<00:00, 14.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Successfully extracted and labeled 108 patches!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_gen.class_indices)  # Should output: {'0': 0, '1': 1}\n",
        "print(val_gen.class_indices)    # Should output: {'0': 0, '1': 1}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2rrRiffV2oF",
        "outputId": "bdc4d01a-0ee9-4ac0-8096-276181606d03"
      },
      "id": "b2rrRiffV2oF",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'0': 0, '1': 1}\n",
            "{'0': 0, '1': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training class distribution:\", dict(zip(*np.unique(train_gen.classes, return_counts=True))))\n",
        "print(\"Validation class distribution:\", dict(zip(*np.unique(val_gen.classes, return_counts=True))))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ci_xKK_XXPpy",
        "outputId": "ee6125bf-9614-4be4-924f-d3a52dbf8a77"
      },
      "id": "Ci_xKK_XXPpy",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training class distribution: {1: 87}\n",
            "Validation class distribution: {1: 21}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Define paths\n",
        "tnt_dir = os.path.join(labeled_dir, \"1\")\n",
        "no_tnt_dir = os.path.join(labeled_dir, \"0\")\n",
        "\n",
        "# Get file lists\n",
        "tnt_files = os.listdir(tnt_dir)\n",
        "no_tnt_files = os.listdir(no_tnt_dir)\n",
        "\n",
        "# Ensure we have both classes\n",
        "if len(no_tnt_files) == 0:\n",
        "    raise ValueError(\"No No-TNT patches found! Ensure extracted both TNT and No-TNT patches.\")\n",
        "\n",
        "# Balance the dataset before splitting\n",
        "num_patches = min(len(tnt_files), len(no_tnt_files))  # Get min count to balance\n",
        "tnt_files = random.sample(tnt_files, num_patches)\n",
        "no_tnt_files = random.sample(no_tnt_files, num_patches)\n",
        "\n",
        "print(f\"✅ Balanced dataset: {num_patches} TNT patches, {num_patches} No-TNT patches\")\n",
        "\n",
        "# Move extra files to a backup folder instead of deleting them\n",
        "backup_dir = os.path.join(base_colab_path, \"backup\")\n",
        "os.makedirs(backup_dir, exist_ok=True)\n",
        "\n",
        "for f in os.listdir(tnt_dir):\n",
        "    if f not in tnt_files:\n",
        "        shutil.move(os.path.join(tnt_dir, f), os.path.join(backup_dir, f))\n",
        "\n",
        "for f in os.listdir(no_tnt_dir):\n",
        "    if f not in no_tnt_files:\n",
        "        shutil.move(os.path.join(no_tnt_dir, f), os.path.join(backup_dir, f))\n",
        "\n",
        "print(f\"✅ After balancing: {len(tnt_files)} TNT patches, {len(no_tnt_files)} No-TNT patches\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wt3Qd8EEXqaP",
        "outputId": "3aac391b-5437-4d27-f775-3996239297af"
      },
      "id": "Wt3Qd8EEXqaP",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Balanced dataset: 108 TNT patches, 108 No-TNT patches\n",
            "✅ After balancing: 108 TNT patches, 108 No-TNT patches\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train VGG16 Model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.metrics import AUC\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "batch_size, image_size = 8, (224, 224)\n",
        "\n"
      ],
      "metadata": {
        "id": "K4g7ceIjyj79"
      },
      "id": "K4g7ceIjyj79",
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data generators\n",
        "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "train_gen = datagen.flow_from_directory(\n",
        "    labeled_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=8,\n",
        "    class_mode='binary',\n",
        "    subset='training'  # Training data\n",
        ")\n",
        "val_gen = datagen.flow_from_directory(\n",
        "    labeled_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=8,\n",
        "    class_mode='binary',\n",
        "    subset='validation'  # Validation data\n",
        ")"
      ],
      "metadata": {
        "id": "9P233ekFyqjW",
        "outputId": "0e8298ce-518b-437a-be77-2b5a2fa37bf5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "9P233ekFyqjW",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 174 images belonging to 2 classes.\n",
            "Found 42 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training class distribution:\", dict(zip(*np.unique(train_gen.classes, return_counts=True))))\n",
        "print(\"Validation class distribution:\", dict(zip(*np.unique(val_gen.classes, return_counts=True))))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEB7UPV8YVih",
        "outputId": "dd4e2b6a-ca7b-4048-866f-ff23fcd5774c"
      },
      "id": "oEB7UPV8YVih",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training class distribution: {0: 87, 1: 87}\n",
            "Validation class distribution: {0: 21, 1: 21}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Setup\n",
        "vgg_model = VGG16(include_top=False, pooling='avg', weights='imagenet', input_shape=(224, 224, 3))\n",
        "for layer in vgg_model.layers[:-2]: layer.trainable = False\n",
        "\n",
        "x = Flatten()(vgg_model.output)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "output = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = Model(inputs=vgg_model.input, outputs=output)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', Precision(name='precision'), Recall(name='recall')])"
      ],
      "metadata": {
        "id": "VSjQ9vkiy5PV"
      },
      "id": "VSjQ9vkiy5PV",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train with early stopping\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
        "    ModelCheckpoint(os.path.join(base_colab_path, \"best_tnt_model.keras\"), save_best_only=True)\n",
        "]\n",
        "model.fit(train_gen, validation_data=val_gen, epochs=3, steps_per_epoch=250, validation_steps=50, callbacks=callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6WhI9uK381j",
        "outputId": "15cdf99e-8028-45b8-85a1-658924bd3667"
      },
      "id": "H6WhI9uK381j",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m 22/250\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17:46\u001b[0m 5s/step - accuracy: 0.4529 - loss: 0.7297 - precision: 0.5099 - recall: 0.5679"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 499ms/step - accuracy: 0.4854 - loss: 0.7159 - precision: 0.4900 - recall: 0.4798 - val_accuracy: 0.5000 - val_loss: 0.6974 - val_precision: 0.5000 - val_recall: 0.0476\n",
            "Epoch 2/3\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 497ms/step - accuracy: 0.4599 - loss: 0.7537 - precision: 0.4508 - recall: 0.3833 - val_accuracy: 0.5000 - val_loss: 0.6954 - val_precision: 0.5000 - val_recall: 0.9524\n",
            "Epoch 3/3\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 491ms/step - accuracy: 0.5052 - loss: 0.7272 - precision: 0.5074 - recall: 0.7038 - val_accuracy: 0.5000 - val_loss: 0.7222 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x799e48ab60d0>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction on new images\n",
        "model = tf.keras.models.load_model(os.path.join(base_colab_path, \"best_tnt_model.keras\"))"
      ],
      "metadata": {
        "id": "UR3_nqdW4Glu"
      },
      "id": "UR3_nqdW4Glu",
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict example image\n",
        "import random\n",
        "test_image_path = os.path.join(base_colab_path, \"labeled\", \"1\", random.choice(os.listdir(os.path.join(labeled_dir, \"1\"))))\n",
        "test_img = cv2.imread(test_image_path)\n",
        "test_img = cv2.resize(test_img, (224, 224)).astype('float32') / 255.0\n",
        "prediction = model.predict(np.expand_dims(test_img, axis=0))[0][0]\n",
        "print(\"TNT Detected\" if prediction > 0.5 else \"No TNT Detected\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKuxZ4cV-FhQ",
        "outputId": "71419cb8-eaf8-43f3-e2c7-d5f1f52eddec"
      },
      "id": "uKuxZ4cV-FhQ",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 720ms/step\n",
            "TNT Detected\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11fcf03c-fdbc-4409-9294-2c5c8d1bf6e8",
      "metadata": {
        "id": "11fcf03c-fdbc-4409-9294-2c5c8d1bf6e8"
      },
      "source": [
        "![Screen Shot 2022-04-30 at 5.25.53 AM.png](attachment:5ffe6c33-f176-418f-b0c2-7de8fdb54ca2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4566efb6-1f80-420a-ad54-3722f3793560",
      "metadata": {
        "id": "4566efb6-1f80-420a-ad54-3722f3793560"
      },
      "source": [
        "![image.png](attachment:3c0f5fe4-5dce-4309-9d13-6c0ca1456c23.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56403038-c956-40c8-88ff-67623ad40439",
      "metadata": {
        "id": "56403038-c956-40c8-88ff-67623ad40439"
      },
      "source": [
        "![image.png](attachment:4ed21fdd-484c-47ae-a521-408e48c1d204.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "924a1e21-38c9-4d08-b79a-2965e953a316",
      "metadata": {
        "id": "924a1e21-38c9-4d08-b79a-2965e953a316"
      },
      "source": [
        "![Unknown.png](attachment:fe969990-2e4f-41ad-ba28-eddf83c2b9ed.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb15524f-6314-4cd1-98d5-e180ace81393",
      "metadata": {
        "id": "eb15524f-6314-4cd1-98d5-e180ace81393"
      },
      "source": [
        "\n",
        "# classification model training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1260dc7d-8634-442d-a987-394d3602bb57",
      "metadata": {
        "id": "1260dc7d-8634-442d-a987-394d3602bb57"
      },
      "source": [
        "\n",
        "- Use cv2.threshold and cv2.dilate to create image masks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eee11d95-97df-45d9-8e71-5d047e218dec",
      "metadata": {
        "id": "eee11d95-97df-45d9-8e71-5d047e218dec"
      },
      "source": [
        "![Tiling 11slidew0slideh1024.png](attachment:47006a89-aecb-43af-b961-afd505921a20.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e90cb0e-2181-45fd-937d-c43a1e63aa54",
      "metadata": {
        "id": "1e90cb0e-2181-45fd-937d-c43a1e63aa54"
      },
      "source": [
        "- Use transfer learning to create a classification model for images with and without TNTs\n",
        "- You can start from and use VGG16\n",
        "- Your goal is to run as many as models as the types of images you created in the preprocessing step, that is, if you created images of size 512 and 250 pixels, then you will have two models for each image size."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}